{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DjQdgjnftoFm",
    "outputId": "c9532642-56a9-4657-90b7-dda295c42f8e"
   },
   "outputs": [],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RrGcbLSRqcHa"
   },
   "outputs": [],
   "source": [
    "# 2: Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1OnuqPEaqvX5"
   },
   "outputs": [],
   "source": [
    "# 3: Local Validation Metric (Winkler Score)\n",
    "# This remains the same. A reliable local score is critical.\n",
    "def winkler_score(y_true, lower, upper, alpha=0.1):\n",
    "    score = upper - lower\n",
    "    if y_true < lower:\n",
    "        score += (2 / alpha) * (lower - y_true)\n",
    "    elif y_true > upper:\n",
    "        score += (2 / alpha) * (y_true - upper)\n",
    "    return score\n",
    "\n",
    "def mean_winkler_score(y_true, lower, upper, alpha=0.1):\n",
    "    scores = [winkler_score(yt, l, u, alpha) for yt, l, u in zip(y_true, lower, upper)]\n",
    "    return np.mean(scores) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QK_BNoKrqvUi",
    "outputId": "6184efca-a33e-41e2-b0b3-039cf4f09e46"
   },
   "outputs": [],
   "source": [
    "# 4: Data Loading and Memory Optimization\n",
    "# This also remains the same.\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "try:\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Define project_root as the current working directory\n",
    "    project_root = Path.cwd()\n",
    "    base_dir = project_root / \"data\"\n",
    "\n",
    "    train_path = base_dir / \"dataset.csv\"\n",
    "    test_path  = base_dir / \"test.csv\"\n",
    "\n",
    "    train_df_raw = pd.read_csv(train_path)\n",
    "    test_df      = pd.read_csv(test_path)\n",
    "\n",
    "    train_df_raw = reduce_mem_usage(train_df_raw)\n",
    "    test_df = reduce_mem_usage(test_df)\n",
    "\n",
    "    print(\"Data loaded and memory reduced.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    train_df_raw = pd.DataFrame()\n",
    "    test_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 602
    },
    "id": "el9VJ-Iqud6D",
    "outputId": "2e9a5f3c-9a57-4d88-aaaf-e8707e1f8a5d"
   },
   "outputs": [],
   "source": [
    "# 5: Exploratory Data Analysis (EDA) and Visualizations\n",
    "if not train_df_raw.empty:\n",
    "    # 5.1. Target Variable Analysis\n",
    "    plt.figure(figsize=(18, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(train_df_raw['sale_price'], kde=True, bins=50)\n",
    "    plt.title('Distribution of Original Sale Price')\n",
    "    plt.xlabel('Sale Price')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(np.log1p(train_df_raw['sale_price']), kde=True, bins=50)\n",
    "    plt.title('Distribution of Log-transformed Sale Price')\n",
    "    plt.xlabel('Log(Sale Price + 1)')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    plt.suptitle('Target Variable Distribution Analysis', fontsize=16)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "id": "xw3bJ15Ruff4",
    "outputId": "5399c317-2f3f-450b-f456-67288f932b5a"
   },
   "outputs": [],
   "source": [
    "    # 5.2. Correlation Analysis\n",
    "    df_train_for_corr = train_df_raw.copy()\n",
    "    df_train_for_corr['log_sale_price'] = np.log1p(df_train_for_corr['sale_price'])\n",
    "\n",
    "    corr_matrix = df_train_for_corr.select_dtypes(include=np.number).corr()\n",
    "\n",
    "    top_corr_features = corr_matrix['log_sale_price'].abs().sort_values(ascending=False).index[1:21]\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.barplot(x=corr_matrix['log_sale_price'][top_corr_features], y=top_corr_features)\n",
    "    plt.title('Top 20 Features Correlated with Log-transformed Sale Price')\n",
    "    plt.xlabel('Correlation Coefficient')\n",
    "    plt.ylabel('Feature Name')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nP0UoD6aqvOq",
    "outputId": "86d192e9-c41a-4aea-df68-a2bd41194f50"
   },
   "outputs": [],
   "source": [
    "# 6: Feature Engineering and Preprocessing\n",
    "def feature_engineer(df):\n",
    "    df_copy = df.copy()\n",
    "    # Use a flexible format and coerce errors to NaT (Not a Time)\n",
    "    df_copy['sale_date'] = pd.to_datetime(df_copy['sale_date'], format='mixed', errors='coerce')\n",
    "\n",
    "    # Handle any dates that could not be parsed\n",
    "    # A simple and robust strategy is to fill with the median date from the parsed dates\n",
    "    if df_copy['sale_date'].isnull().any():\n",
    "        median_date = df_copy['sale_date'].median()\n",
    "        df_copy['sale_date'].fillna(median_date, inplace=True)\n",
    "\n",
    "    df_copy['sale_year'] = df_copy['sale_date'].dt.year\n",
    "    df_copy['sale_month'] = df_copy['sale_date'].dt.month\n",
    "    df_copy['sale_dayofyear'] = df_copy['sale_date'].dt.dayofyear\n",
    "    df_copy['house_age'] = df_copy['sale_year'] - df_copy['year_built']\n",
    "    df_copy['years_since_reno'] = np.where((df_copy['year_reno'] > 0) & (df_copy['year_reno'] >= df_copy['year_built']), df_copy['sale_year'] - df_copy['year_reno'], 0)\n",
    "    df_copy['was_renovated'] = (df_copy['years_since_reno'] > 0).astype(int)\n",
    "    df_copy = df_copy.drop(columns=['sale_date', 'year_built', 'year_reno'])\n",
    "    return df_copy\n",
    "\n",
    "if not train_df_raw.empty:\n",
    "    train_df_raw['log_sale_price'] = np.log1p(train_df_raw['sale_price'])\n",
    "    TARGET = 'log_sale_price'\n",
    "    FEATURES = [col for col in train_df_raw.columns if col not in ['id', 'sale_price', 'log_sale_price']]\n",
    "\n",
    "    X = train_df_raw[FEATURES]\n",
    "    y = train_df_raw[TARGET]\n",
    "    X_test = test_df[[col for col in FEATURES if col in test_df.columns]]\n",
    "\n",
    "    X = feature_engineer(X)\n",
    "    X_test = feature_engineer(X_test)\n",
    "\n",
    "    numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "    categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "    # CRITICAL FIX: Fill NaN values in categorical features before type conversion.\n",
    "    # CatBoost cannot handle NaN in categorical columns.\n",
    "    for col in categorical_features:\n",
    "        X[col] = X[col].fillna('missing')\n",
    "        X_test[col] = X_test[col].fillna('missing')\n",
    "\n",
    "    for col in categorical_features:\n",
    "        X[col] = X[col].astype('category')\n",
    "        X_test[col] = X_test[col].astype('category')\n",
    "\n",
    "    preprocessor = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "    X[numerical_features] = preprocessor.fit_transform(X[numerical_features])\n",
    "    X_test[numerical_features] = preprocessor.transform(X_test[numerical_features])\n",
    "\n",
    "    print(\"Preprocessing complete.\")\n",
    "    del train_df_raw\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gf-pt6v5qvL7",
    "outputId": "4a813a84-79cb-435f-be69-dcfd283e40d7"
   },
   "outputs": [],
   "source": [
    "# 7: Data Splitting for Conformal Prediction\n",
    "if 'X' in locals() and not X.empty:\n",
    "    X_train_proper, X_calib, y_train_proper, y_calib = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(\"Data split into training and calibration sets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bU_rfy_uqvJT",
    "outputId": "6f4877cf-ae3e-49c0-d9f9-1e20b8b7735e"
   },
   "outputs": [],
   "source": [
    "# 8: Model Training\n",
    "# Train all 6 models (3 base models x 2 quantiles).\n",
    "if 'X_train_proper' in locals():\n",
    "    alpha = 0.1\n",
    "    q_lower = alpha / 2.0\n",
    "    q_upper = 1 - (alpha / 2.0)\n",
    "\n",
    "    # --- LightGBM ---\n",
    "    print(\"Training LightGBM models...\")\n",
    "    lgbm_params = {'objective': 'quantile', 'metric': 'quantile', 'random_state': 42, 'n_estimators': 1000}\n",
    "    lgbm_lower = lgb.LGBMRegressor(alpha=q_lower, **lgbm_params)\n",
    "    lgbm_upper = lgb.LGBMRegressor(alpha=q_upper, **lgbm_params)\n",
    "    lgbm_lower.fit(X_train_proper, y_train_proper, categorical_feature=categorical_features, eval_set=[(X_calib, y_calib)], callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "    lgbm_upper.fit(X_train_proper, y_train_proper, categorical_feature=categorical_features, eval_set=[(X_calib, y_calib)], callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "\n",
    "    # --- XGBoost ---\n",
    "    print(\"Training XGBoost models...\")\n",
    "    # Convert categorical features to integer codes for XGBoost\n",
    "    X_train_xgb = X_train_proper.copy()\n",
    "    X_calib_xgb = X_calib.copy()\n",
    "    for col in categorical_features:\n",
    "        X_train_xgb[col] = X_train_xgb[col].cat.codes\n",
    "        X_calib_xgb[col] = X_calib_xgb[col].cat.codes\n",
    "    xgb_params = {'objective': 'reg:quantileerror', 'random_state': 42, 'n_estimators': 1000, 'tree_method': 'hist'}\n",
    "    xgb_lower = xgb.XGBRegressor(quantile_alpha=q_lower, **xgb_params)\n",
    "    xgb_upper = xgb.XGBRegressor(quantile_alpha=q_upper, **xgb_params)\n",
    "    # This simplified fit call is more robust across different environments.\n",
    "    xgb_lower.fit(X_train_xgb, y_train_proper, eval_set=[(X_calib_xgb, y_calib)], verbose=False)\n",
    "    xgb_upper.fit(X_train_xgb, y_train_proper, eval_set=[(X_calib_xgb, y_calib)], verbose=False)\n",
    "\n",
    "    # --- CatBoost ---\n",
    "    print(\"Training CatBoost models...\")\n",
    "    # Option 1: Disable logging completely by setting train_dir=None\n",
    "    cat_params_lower = {'loss_function': f'Quantile:alpha={q_lower}', 'iterations': 1000, 'verbose': 0, 'random_seed': 42, 'train_dir': None}\n",
    "    cat_params_upper = {'loss_function': f'Quantile:alpha={q_upper}', 'iterations': 1000, 'verbose': 0, 'random_seed': 42, 'train_dir': None}\n",
    "\n",
    "    # Option 2: Or specify a custom directory (uncomment the lines below and comment out the ones above)\n",
    "    # cat_params_lower = {'loss_function': f'Quantile:alpha={q_lower}', 'iterations': 1000, 'verbose': 0, 'random_seed': 42, 'train_dir': './ml_model/catboost_logs'}\n",
    "    # cat_params_upper = {'loss_function': f'Quantile:alpha={q_upper}', 'iterations': 1000, 'verbose': 0, 'random_seed': 42, 'train_dir': './ml_model/catboost_logs'}\n",
    "\n",
    "    cat_lower = cb.CatBoostRegressor(**cat_params_lower)\n",
    "    cat_upper = cb.CatBoostRegressor(**cat_params_upper)\n",
    "\n",
    "    cat_lower.fit(X_train_proper, y_train_proper, cat_features=categorical_features, eval_set=[(X_calib, y_calib)], early_stopping_rounds=100)\n",
    "    cat_upper.fit(X_train_proper, y_train_proper, cat_features=categorical_features, eval_set=[(X_calib, y_calib)], early_stopping_rounds=100)\n",
    "\n",
    "    print(\"All models trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BRZVra5qqvAP",
    "outputId": "241297e3-fbe1-45a3-b65a-492f2802d752"
   },
   "outputs": [],
   "source": [
    "# 9: Ensemble CQR Calibration and Evaluation\n",
    "if 'lgbm_lower' in locals():\n",
    "    # 1. Get predictions from all models on the calibration set\n",
    "    lgbm_cal_preds_lower = lgbm_lower.predict(X_calib)\n",
    "    lgbm_cal_preds_upper = lgbm_upper.predict(X_calib)\n",
    "    xgb_cal_preds_lower = xgb_lower.predict(X_calib_xgb) # Use encoded data for XGBoost\n",
    "    xgb_cal_preds_upper = xgb_upper.predict(X_calib_xgb) # Use encoded data for XGBoost\n",
    "    cat_cal_preds_lower = cat_lower.predict(X_calib)\n",
    "    cat_cal_preds_upper = cat_upper.predict(X_calib)\n",
    "\n",
    "    # 2. Average the predictions to create the ensemble interval\n",
    "    ensemble_cal_lower = np.mean([lgbm_cal_preds_lower, xgb_cal_preds_lower, cat_cal_preds_lower], axis=0)\n",
    "    ensemble_cal_upper = np.mean([lgbm_cal_preds_upper, xgb_cal_preds_upper, cat_cal_preds_upper], axis=0)\n",
    "\n",
    "    # 3. Calculate conformity scores for the ENSEMBLE\n",
    "    conformity_scores = np.maximum(ensemble_cal_lower - y_calib, y_calib - ensemble_cal_upper)\n",
    "\n",
    "    # 4. Calculate the correction term 'q'\n",
    "    n_calib = len(X_calib)\n",
    "    q_level = np.ceil((1 - alpha) * (n_calib + 1)) / n_calib\n",
    "    q = np.quantile(conformity_scores, q_level, interpolation='higher')\n",
    "    print(f\"Calculated Ensemble CQR correction factor (q): {q:.4f}\")\n",
    "\n",
    "    # 5. Apply correction and evaluate\n",
    "    final_lower_bound_log = ensemble_cal_lower - q\n",
    "    final_upper_bound_log = ensemble_cal_upper + q\n",
    "\n",
    "    y_calib_orig = np.expm1(y_calib)\n",
    "    final_lower_bound_orig = np.expm1(final_lower_bound_log)\n",
    "    final_upper_bound_orig = np.expm1(final_upper_bound_log)\n",
    "\n",
    "    coverage = np.mean((y_calib_orig >= final_lower_bound_orig) & (y_calib_orig <= final_upper_bound_orig))\n",
    "    winkler = mean_winkler_score(y_calib_orig, final_lower_bound_orig, final_upper_bound_orig)\n",
    "\n",
    "    print(f\"\\nTarget Coverage: {1-alpha:.2f}\")\n",
    "    print(f\"Ensemble Empirical Coverage on Hold-out Set: {coverage:.4f}\")\n",
    "    print(f\"Ensemble Mean Winkler Score on Hold-out Set: {winkler:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "id": "rdZduLzLr-BK",
    "outputId": "a8357d2a-86be-455f-c495-68ecc00b5b05"
   },
   "outputs": [],
   "source": [
    "# 10: Model Performance Visualization\n",
    "if 'y_calib_orig' in locals():\n",
    "    # 10.1. Prediction Interval Visualization\n",
    "    results_df = pd.DataFrame({\n",
    "        'y_true': y_calib_orig,\n",
    "        'lower': final_lower_bound_orig,\n",
    "        'upper': final_upper_bound_orig\n",
    "    }).sort_values('y_true').reset_index(drop=True)\n",
    "\n",
    "    sample_results = results_df.sample(n=min(500, len(results_df)), random_state=42).sort_values('y_true')\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.scatter(range(len(sample_results)), sample_results['y_true'], color='blue', label='True Values', s=15, zorder=3)\n",
    "    plt.fill_between(\n",
    "        range(len(sample_results)),\n",
    "        sample_results['lower'],\n",
    "        sample_results['upper'],\n",
    "        color='orange',\n",
    "        alpha=0.4,\n",
    "        label='90% Prediction Interval',\n",
    "        zorder=1\n",
    "    )\n",
    "    plt.title('CQR Prediction Intervals vs. True Values (Sample of 500, Sorted)')\n",
    "    plt.xlabel('Sample Index (Sorted by True Value)')\n",
    "    plt.ylabel('Sale Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 769
    },
    "id": "KzaWN6lvzQkk",
    "outputId": "d6d5e8c6-f448-48a2-c089-0980bb1aff56"
   },
   "outputs": [],
   "source": [
    "\n",
    "    # 10.2. Feature Importance Plots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(24, 10))\n",
    "\n",
    "    # LightGBM Feature Importance\n",
    "    lgbm_importances = pd.DataFrame({'feature': X.columns, 'importance': lgbm_upper.feature_importances_}).sort_values('importance', ascending=False).head(20)\n",
    "    sns.barplot(x='importance', y='feature', data=lgbm_importances, ax=axes[0], palette='viridis')\n",
    "    axes[0].set_title('Top 20 Features (LightGBM)')\n",
    "\n",
    "    # XGBoost Feature Importance\n",
    "    xgb_importances = pd.DataFrame({'feature': X.columns, 'importance': xgb_upper.feature_importances_}).sort_values('importance', ascending=False).head(20)\n",
    "    sns.barplot(x='importance', y='feature', data=xgb_importances, ax=axes[1], palette='plasma')\n",
    "    axes[1].set_title('Top 20 Features (XGBoost)')\n",
    "\n",
    "    # CatBoost Feature Importance\n",
    "    cat_importances = pd.DataFrame({'feature': X.columns, 'importance': cat_upper.feature_importances_}).sort_values('importance', ascending=False).head(20)\n",
    "    sns.barplot(x='importance', y='feature', data=cat_importances, ax=axes[2], palette='magma')\n",
    "    axes[2].set_title('Top 20 Features (CatBoost)')\n",
    "\n",
    "    plt.suptitle('Comparison of Feature Importances Across Models', fontsize=20)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GRqEogPtzRoY",
    "outputId": "f2967179-60d7-4b85-f1b2-2b2c951ccfa6"
   },
   "outputs": [],
   "source": [
    "    # 10.3. Model Agreement Scatter Plots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(22, 12))\n",
    "\n",
    "    # Lower Bound Comparisons\n",
    "    axes[0, 0].scatter(lgbm_cal_preds_lower, cat_cal_preds_lower, alpha=0.2)\n",
    "    axes[0, 0].set_title('Lower Bound: LGBM vs CatBoost')\n",
    "    axes[0, 1].scatter(lgbm_cal_preds_lower, xgb_cal_preds_lower, alpha=0.2)\n",
    "    axes[0, 1].set_title('Lower Bound: LGBM vs XGBoost')\n",
    "    axes[0, 2].scatter(cat_cal_preds_lower, xgb_cal_preds_lower, alpha=0.2)\n",
    "    axes[0, 2].set_title('Lower Bound: CatBoost vs XGBoost')\n",
    "\n",
    "    # Upper Bound Comparisons\n",
    "    axes[1, 0].scatter(lgbm_cal_preds_upper, cat_cal_preds_upper, alpha=0.2)\n",
    "    axes[1, 0].set_title('Upper Bound: LGBM vs CatBoost')\n",
    "    axes[1, 1].scatter(lgbm_cal_preds_upper, xgb_cal_preds_upper, alpha=0.2)\n",
    "    axes[1, 1].set_title('Upper Bound: LGBM vs XGBoost')\n",
    "    axes[1, 2].scatter(cat_cal_preds_upper, xgb_cal_preds_upper, alpha=0.2)\n",
    "    axes[1, 2].set_title('Upper Bound: CatBoost vs XGBoost')\n",
    "\n",
    "    for ax_row in axes:\n",
    "        for ax in ax_row:\n",
    "            lims = [\n",
    "                np.min([ax.get_xlim(), ax.get_ylim()]),\n",
    "                np.max([ax.get_xlim(), ax.get_ylim()]),\n",
    "            ]\n",
    "            ax.plot(lims, lims, 'r--', alpha=0.75, zorder=0)\n",
    "            ax.set_aspect('equal', adjustable='box')\n",
    "            ax.grid(True)\n",
    "\n",
    "    plt.suptitle('Pairwise Comparison of Model Predictions on Calibration Set (Log Scale)', fontsize=20)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tww812por91d",
    "outputId": "16c02b12-aafb-40a6-cd26-198ae22c02d3"
   },
   "outputs": [],
   "source": [
    "# 11: Final Training and Submission File Generation\n",
    "if 'X' in locals():\n",
    "    print(\"Retraining all models on the full dataset...\")\n",
    "\n",
    "    lgbm_params_final = {'objective': 'quantile', 'metric': 'quantile', 'random_state': 42, 'n_estimators': lgbm_lower.best_iteration_}\n",
    "    final_lgbm_lower = lgb.LGBMRegressor(alpha=q_lower, **lgbm_params_final).fit(X, y, categorical_feature=categorical_features)\n",
    "    lgbm_params_final['n_estimators'] = lgbm_upper.best_iteration_\n",
    "    final_lgbm_upper = lgb.LGBMRegressor(alpha=q_upper, **lgbm_params_final).fit(X, y, categorical_feature=categorical_features)\n",
    "\n",
    "    # Prepare full data for final XGBoost training\n",
    "    X_xgb_full = X.copy()\n",
    "    X_test_xgb = X_test.copy()\n",
    "    for col in categorical_features:\n",
    "        X_xgb_full[col] = X_xgb_full[col].cat.codes\n",
    "        X_test_xgb[col] = X_test_xgb[col].cat.codes\n",
    "\n",
    "    # Train final XGBoost models on the full encoded data\n",
    "    xgb_params_final = {'objective': 'reg:quantileerror', 'random_state': 42, 'n_estimators': 1000, 'tree_method': 'hist'}\n",
    "    final_xgb_lower = xgb.XGBRegressor(quantile_alpha=q_lower, **xgb_params_final).fit(X_xgb_full, y)\n",
    "    final_xgb_upper = xgb.XGBRegressor(quantile_alpha=q_upper, **xgb_params_final).fit(X_xgb_full, y)\n",
    "\n",
    "    # CatBoost final training with controlled logging\n",
    "    cat_params_final_lower = {'loss_function': f'Quantile:alpha={q_lower}', 'iterations': cat_lower.get_best_iteration(), 'verbose': 0, 'random_seed': 42, 'train_dir': None}\n",
    "    final_cat_lower = cb.CatBoostRegressor(**cat_params_final_lower).fit(X, y, cat_features=categorical_features)\n",
    "\n",
    "    cat_params_final_upper = {'loss_function': f'Quantile:alpha={q_upper}', 'iterations': cat_upper.get_best_iteration(), 'verbose': 0, 'random_seed': 42, 'train_dir': None}\n",
    "    final_cat_upper = cb.CatBoostRegressor(**cat_params_final_upper).fit(X, y, cat_features=categorical_features)\n",
    "\n",
    "    print(\"Final model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276
    },
    "id": "U3MpzrGEsaB2",
    "outputId": "8d794e92-1f4b-4a91-c327-1f38102220a9"
   },
   "outputs": [],
   "source": [
    "    # --- Generate Submission ---\n",
    "    print(\"Generating predictions for the test set...\")\n",
    "    lgbm_test_lower = final_lgbm_lower.predict(X_test)\n",
    "    lgbm_test_upper = final_lgbm_upper.predict(X_test)\n",
    "    xgb_test_lower = final_xgb_lower.predict(X_test_xgb)\n",
    "    xgb_test_upper = final_xgb_upper.predict(X_test_xgb)\n",
    "    cat_test_lower = final_cat_lower.predict(X_test)\n",
    "    cat_test_upper = final_cat_upper.predict(X_test)\n",
    "\n",
    "    # Ensemble the test predictions\n",
    "    ensemble_test_lower = np.mean([lgbm_test_lower, xgb_test_lower, cat_test_lower], axis=0)\n",
    "    ensemble_test_upper = np.mean([lgbm_test_upper, xgb_test_upper, cat_test_upper], axis=0)\n",
    "\n",
    "    # Apply the correction factor 'q' found during calibration\n",
    "    final_test_lower_log = ensemble_test_lower - q\n",
    "    final_test_upper_log = ensemble_test_upper + q\n",
    "\n",
    "    # Transform back to original scale\n",
    "    final_test_lower_orig = np.expm1(final_test_lower_log)\n",
    "    final_test_upper_orig = np.expm1(final_test_upper_log)\n",
    "\n",
    "    # Post-processing\n",
    "    final_test_lower_orig = np.maximum(0, final_test_lower_orig)\n",
    "    final_test_upper_orig = np.maximum(final_test_lower_orig, final_test_upper_orig)\n",
    "\n",
    "    # Create submission file\n",
    "    test_df_ids = pd.read_csv(test_path, usecols=['id'])\n",
    "    submission_df = pd.DataFrame({'id': test_df_ids['id'], 'pi_lower': final_test_lower_orig, 'pi_upper': final_test_upper_orig})\n",
    "\n",
    "    submission_filename = '20282106_ensemble_submission-v6.csv'\n",
    "    submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "    print(f\"\\nSubmission file '{submission_filename}' created successfully!\")\n",
    "    print(f\"Saved to: {os.path.abspath(submission_filename)}\")\n",
    "\n",
    "    try:\n",
    "        display(submission_df.head())\n",
    "    except NameError:\n",
    "        print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12: Save Models and Preprocessor for Application ---\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(\"Saving models and artifacts...\")\n",
    "\n",
    "# Define the directory to save the artifacts\n",
    "artifact_path = os.path.join(os.getcwd(), 'ml_model')\n",
    "os.makedirs(artifact_path, exist_ok=True)\n",
    "\n",
    "# Save the 6 trained models\n",
    "joblib.dump(final_lgbm_lower, os.path.join(artifact_path, 'lgbm_lower.pkl'))\n",
    "joblib.dump(final_lgbm_upper, os.path.join(artifact_path, 'lgbm_upper.pkl'))\n",
    "joblib.dump(final_xgb_lower, os.path.join(artifact_path, 'xgb_lower.pkl'))\n",
    "joblib.dump(final_xgb_upper, os.path.join(artifact_path, 'xgb_upper.pkl'))\n",
    "joblib.dump(final_cat_lower, os.path.join(artifact_path, 'cat_lower.pkl'))\n",
    "joblib.dump(final_cat_upper, os.path.join(artifact_path, 'cat_upper.pkl'))\n",
    "\n",
    "# Save the preprocessor pipeline\n",
    "joblib.dump(preprocessor, os.path.join(artifact_path, 'preprocessor.pkl'))\n",
    "\n",
    "# Save the feature lists and the 'q' value in a JSON file for easy loading\n",
    "artifacts_to_save = {\n",
    "    'q_value': q,\n",
    "    'numerical_features': numerical_features,\n",
    "    'categorical_features': categorical_features,\n",
    "    'features_order': list(X.columns) # Save the exact order of columns\n",
    "}\n",
    "\n",
    "with open(os.path.join(artifact_path, 'model_config.json'), 'w') as f:\n",
    "    json.dump(artifacts_to_save, f)\n",
    "\n",
    "print(f\"All artifacts saved to {artifact_path}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
